# Emotion-Recognition-Through-Speech-Using-CNN
# Speech-Emotion-Recognition-Through-Speech-using-CNN

The objective is to build a speech emotion detection classifier.

# What is Speech Emotion Recognition.
Speech Emotion Recognition, abbreviated as SER, is the act of attempting to recognize human emotion and affective states from speech. This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.

# Datasets used in this project
1. Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D)
2. Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)
3. Surrey Audio-Visual Expressed Emotion (Savee)
4. Toronto emotional speech set (Tess)

# Methodology
1. Data Collection, Visualization and Exploration: Gathering relevant audio data and visually exploring it to understand patterns and insights.
2. Data Augmentation: Increasing the diversity and size of the        dataset by applying transformations like noise injection, shifting time, changing pitch and speed, etc.
3. Feature Extraction: Identifying and extracting relevant features from the most informative data for the model.
4. Data Preparation: Pre-processing the data by cleaning, normalizing, and formatting it to ensure it is suitable for training the model.
5. Training the model: Using CNN algorithm to learn patterns from the prepared data.
6. Testing and Evaluation: The model's performance on unseen data is assessed to validate its accuracy and effectiveness for the intended task.
